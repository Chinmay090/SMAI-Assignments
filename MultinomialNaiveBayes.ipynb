{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1bd3a4e9-5e71-4d5f-8582-c2141e186370",
   "metadata": {},
   "source": [
    "# SMAI Assignment - 2\n",
    "\n",
    "## Question - `3` : Multinomial Naïve Bayes\n",
    "\n",
    "| | |\n",
    "|- | -|\n",
    "| Course | Statistical Methods in AI |\n",
    "| Release Date | `16.02.2023` |\n",
    "| Due Date | `24.02.2023` |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fcb7b3-fc94-4ce9-b058-0b26a3b93f7a",
   "metadata": {},
   "source": [
    "This question will have you working and experimenting with the Multinomial Naïve Bayes classifier. Initially, you will transform the given data in csv file to count matrix, then calculate the priors. Use those priors to compute likelyhoods according to Multinomial Naive Bayes and then classify the test data. Please note that use of `sklearn` implementations is only for the final question of the assignment, for other doubts regarding libraries you can reach out to the TAs.\n",
    "\n",
    "The dataset is about `Spam SMS`. There is 1 attribute that is the `message`, and the class label which could be `spam` or `ham`. The data is present in `spam.csv`. It contains about 5-6000 samples.\n",
    "For your convinience the data is already pre-processed and loaded, but I suggest you to just take a look at the code for your own knowledge, and parts vectorization is left up to you which could be easily done with the help of the given example code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "aa8a96d4-58f3-4360-b6ae-a02405ffdddb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630170e6-57f8-4dae-b644-ab275a3f53a2",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Reading text-based data using pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2430d4f2-e3ed-4e09-9e50-a446889d58cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './spam.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Chinmay\\Desktop\\3-2\\SMAI\\Assignments\\SMAI_Assignment_2\\2020102069_Assignment 2\\2020102069_Assignment 2\\Q3.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# read file into pandas using a relative path\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m df \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39;49mread_csv(\u001b[39m\"\u001b[39;49m\u001b[39m./spam.csv\u001b[39;49m\u001b[39m\"\u001b[39;49m, encoding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mlatin-1\u001b[39;49m\u001b[39m'\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m df\u001b[39m.\u001b[39mdropna(how\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39many\u001b[39m\u001b[39m\"\u001b[39m, inplace\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m df\u001b[39m.\u001b[39mcolumns \u001b[39m=\u001b[39m [\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mmessage\u001b[39m\u001b[39m'\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\util\\_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(args) \u001b[39m>\u001b[39m num_allow_args:\n\u001b[0;32m    306\u001b[0m     warnings\u001b[39m.\u001b[39mwarn(\n\u001b[0;32m    307\u001b[0m         msg\u001b[39m.\u001b[39mformat(arguments\u001b[39m=\u001b[39marguments),\n\u001b[0;32m    308\u001b[0m         \u001b[39mFutureWarning\u001b[39;00m,\n\u001b[0;32m    309\u001b[0m         stacklevel\u001b[39m=\u001b[39mstacklevel,\n\u001b[0;32m    310\u001b[0m     )\n\u001b[1;32m--> 311\u001b[0m \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:680\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    665\u001b[0m kwds_defaults \u001b[39m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    666\u001b[0m     dialect,\n\u001b[0;32m    667\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    676\u001b[0m     defaults\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39mdelimiter\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39m,\u001b[39m\u001b[39m\"\u001b[39m},\n\u001b[0;32m    677\u001b[0m )\n\u001b[0;32m    678\u001b[0m kwds\u001b[39m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 680\u001b[0m \u001b[39mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:575\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    572\u001b[0m _validate_names(kwds\u001b[39m.\u001b[39mget(\u001b[39m\"\u001b[39m\u001b[39mnames\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m))\n\u001b[0;32m    574\u001b[0m \u001b[39m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 575\u001b[0m parser \u001b[39m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwds)\n\u001b[0;32m    577\u001b[0m \u001b[39mif\u001b[39;00m chunksize \u001b[39mor\u001b[39;00m iterator:\n\u001b[0;32m    578\u001b[0m     \u001b[39mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:934\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m    931\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39moptions[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m kwds[\u001b[39m\"\u001b[39m\u001b[39mhas_index_names\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[0;32m    933\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles: IOHandles \u001b[39m|\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 934\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_engine \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_engine(f, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mengine)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\parsers\\readers.py:1218\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1214\u001b[0m     mode \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1215\u001b[0m \u001b[39m# error: No overload variant of \"get_handle\" matches argument types\u001b[39;00m\n\u001b[0;32m   1216\u001b[0m \u001b[39m# \"Union[str, PathLike[str], ReadCsvBuffer[bytes], ReadCsvBuffer[str]]\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m \u001b[39m# , \"str\", \"bool\", \"Any\", \"Any\", \"Any\", \"Any\", \"Any\"\u001b[39;00m\n\u001b[1;32m-> 1218\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39m=\u001b[39m get_handle(  \u001b[39m# type: ignore[call-overload]\u001b[39;49;00m\n\u001b[0;32m   1219\u001b[0m     f,\n\u001b[0;32m   1220\u001b[0m     mode,\n\u001b[0;32m   1221\u001b[0m     encoding\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1222\u001b[0m     compression\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mcompression\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1223\u001b[0m     memory_map\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mmemory_map\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mFalse\u001b[39;49;00m),\n\u001b[0;32m   1224\u001b[0m     is_text\u001b[39m=\u001b[39;49mis_text,\n\u001b[0;32m   1225\u001b[0m     errors\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mencoding_errors\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39m\"\u001b[39;49m\u001b[39mstrict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[0;32m   1226\u001b[0m     storage_options\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moptions\u001b[39m.\u001b[39;49mget(\u001b[39m\"\u001b[39;49m\u001b[39mstorage_options\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m   1227\u001b[0m )\n\u001b[0;32m   1228\u001b[0m \u001b[39massert\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m   1229\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mhandles\u001b[39m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\pandas\\io\\common.py:786\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    781\u001b[0m \u001b[39melif\u001b[39;00m \u001b[39misinstance\u001b[39m(handle, \u001b[39mstr\u001b[39m):\n\u001b[0;32m    782\u001b[0m     \u001b[39m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[39m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[39mif\u001b[39;00m ioargs\u001b[39m.\u001b[39mencoding \u001b[39mand\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mb\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m ioargs\u001b[39m.\u001b[39mmode:\n\u001b[0;32m    785\u001b[0m         \u001b[39m# Encoding\u001b[39;00m\n\u001b[1;32m--> 786\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(\n\u001b[0;32m    787\u001b[0m             handle,\n\u001b[0;32m    788\u001b[0m             ioargs\u001b[39m.\u001b[39;49mmode,\n\u001b[0;32m    789\u001b[0m             encoding\u001b[39m=\u001b[39;49mioargs\u001b[39m.\u001b[39;49mencoding,\n\u001b[0;32m    790\u001b[0m             errors\u001b[39m=\u001b[39;49merrors,\n\u001b[0;32m    791\u001b[0m             newline\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39m\"\u001b[39;49m,\n\u001b[0;32m    792\u001b[0m         )\n\u001b[0;32m    793\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    794\u001b[0m         \u001b[39m# Binary mode\u001b[39;00m\n\u001b[0;32m    795\u001b[0m         handle \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(handle, ioargs\u001b[39m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './spam.csv'"
     ]
    }
   ],
   "source": [
    "# read file into pandas using a relative path\n",
    "\n",
    "df = pd.read_csv(\"./spam.csv\", encoding='latin-1')\n",
    "df.dropna(how=\"any\", inplace=True, axis=1)\n",
    "df.columns = ['label', 'message']\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c5fbeec1-2b88-4af3-b8c6-906936938951",
   "metadata": {},
   "source": [
    "## Pre-processing\n",
    "\n",
    "- Our main issue with our data is that it is all in text format (strings). The classification algorithms that we usally use need some sort of numerical feature vector in order to perform the classification task. There are actually many methods to convert a corpus to a vector format. The simplest is the bag-of-words approach, where each unique word in a text will be represented by one number.\n",
    "\n",
    "- As a first step, let's write a function that will split a message into its individual words and return a list. We'll also remove very common words, ('the', 'a', etc..). To do this we will take advantage of the NLTK library. It's pretty much the standard library in Python for processing text and has a lot of useful features. We'll only use some of the basic ones here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d736a35c-e849-4029-bee7-42a31f7d0b11",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading stopwords: <urlopen error [Errno 11001]\n",
      "[nltk_data]     getaddrinfo failed>\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    STOPWORDS = stopwords.words('english') + ['u', 'ü', 'ur', '4', '2', 'im', 'dont', 'doin', 'ure']\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return ' '.join([word for word in nopunc.split() if word.lower() not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed807b40-7762-4968-b546-2f72817beed3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>ham</td>\n",
       "      <td>Go jurong point crazy Available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>ham</td>\n",
       "      <td>Ok lar Joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>spam</td>\n",
       "      <td>Free entry wkly comp win FA Cup final tkts 21s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ham</td>\n",
       "      <td>dun say early hor c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ham</td>\n",
       "      <td>Nah think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  label                                            message\n",
       "0   ham  Go jurong point crazy Available bugis n great ...\n",
       "1   ham                              Ok lar Joking wif oni\n",
       "2  spam  Free entry wkly comp win FA Cup final tkts 21s...\n",
       "3   ham                    dun say early hor c already say\n",
       "4   ham             Nah think goes usf lives around though"
      ]
     },
     "execution_count": 754,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['message'] = df.message.apply(text_process)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad88d89f-1452-4e7a-9f9c-cb1094f890d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>message</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Go jurong point crazy Available bugis n great ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>Ok lar Joking wif oni</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>Free entry wkly comp win FA Cup final tkts 21s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>dun say early hor c already say</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nah think goes usf lives around though</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label                                            message\n",
       "0      0  Go jurong point crazy Available bugis n great ...\n",
       "1      0                              Ok lar Joking wif oni\n",
       "2      1  Free entry wkly comp win FA Cup final tkts 21s...\n",
       "3      0                    dun say early hor c already say\n",
       "4      0             Nah think goes usf lives around though"
      ]
     },
     "execution_count": 755,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'] = df.label.map({'ham':0, 'spam':1})\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f22eddba-546a-4e97-b013-2906e90946b7",
   "metadata": {},
   "source": [
    "## Splitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f859993a-2b29-4baf-a168-117151ed240b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X: (5572,)\n",
      "y: (5572,)\n",
      "\n",
      "X_train: (4179,)\n",
      "y_train: (4179,)\n",
      "\n",
      "X_test: (1393,)\n",
      "y_test: (1393,)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# split X and y into training and testing sets \n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df.message\n",
    "y = df.label\n",
    "\n",
    "print(f'X: {X.shape}')\n",
    "print(f'y: {y.shape}')\n",
    "print()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=1)\n",
    "\n",
    "print(f'X_train: {X_train.shape}')\n",
    "print(f'y_train: {y_train.shape}')\n",
    "print()\n",
    "\n",
    "print(f'X_test: {X_test.shape}')\n",
    "print(f'y_test: {y_test.shape}')\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba6deffc-0424-4cb6-85d2-eb74f97e8cdd",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Helper code / Example code for Representing text as Numerical data using Sci-kit learn\n",
    "\n",
    "📌 From the scikit-learn documentation:\n",
    "- Text Analysis is a major application field for machine learning algorithms. However the raw data, a sequence of symbols cannot be fed directly to the algorithms themselves as most of them expect numerical feature vectors with a fixed size rather than the raw text documents with variable length.\n",
    "- We will use CountVectorizer to \"convert text into a matrix of token counts\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1309604c-b9c1-4910-ab60-432f7e12824b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# example text for model training (SMS messages)\n",
    "simple_train = ['call you tonight', 'Call me a cab', 'Please call me... PLEASE!']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fde24ff-b303-4526-899d-fb902eabc1d8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cab', 'call', 'me', 'please', 'tonight', 'you'], dtype=object)"
      ]
     },
     "execution_count": 758,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# import and instantiate CountVectorizer (with the default parameters)\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vect = CountVectorizer()\n",
    "simple_train = vect.fit_transform(simple_train)\n",
    "\n",
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576f6e05-6e94-45a1-96cc-3e694cc971d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cab', 'call', 'me', 'please', 'tonight', 'you'], dtype=object)"
      ]
     },
     "execution_count": 759,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vect.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aef831c-50f0-4723-a04a-27af774f0542",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 0, 0, 1, 1],\n",
       "       [1, 1, 1, 0, 0, 0],\n",
       "       [0, 1, 1, 2, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 760,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# convert sparse matrix to a dense matrix\n",
    "simple_train.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa523614-ff0d-4f22-b8f2-4505e33a2460",
   "metadata": {},
   "source": [
    "In this scheme, features and samples are defined as follows:\n",
    "\n",
    "- Each individual token occurrence frequency (normalized or not) is treated as a feature.\n",
    "- The vector of all the token frequencies for a given document is considered a multivariate sample.\n",
    "\n",
    "A corpus of documents can thus be represented by a matrix with one row per document and one column per token (e.g. word) occurring in the corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d0a1561-6e5c-495e-b056-fc9fe376121d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   0       0        1    1\n",
       "1    1     1   1       0        0    0\n",
       "2    0     1   1       2        0    0"
      ]
     },
     "execution_count": 761,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_train.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02042133-9bad-4aa3-b0be-a53158e7102e",
   "metadata": {},
   "source": [
    "### Transform Testing data into a document-term matrix (using existing / training vocabulary)\n",
    "\n",
    "- You are supposed to use the training vocabolary to support your mother"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a50c164-91ac-4861-993d-e14a2be7c6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "simple_test = [\"please don't call me\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30e0b5eb-d7ab-468a-86e2-89dcd0fa0184",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 1, 1, 1, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 763,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "simple_test_dtm = vect.transform(simple_test)\n",
    "simple_test_dtm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df0574c-6088-4418-92b4-d6d69c9d9d41",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cab</th>\n",
       "      <th>call</th>\n",
       "      <th>me</th>\n",
       "      <th>please</th>\n",
       "      <th>tonight</th>\n",
       "      <th>you</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   cab  call  me  please  tonight  you\n",
       "0    0     1   1       1        0    0"
      ]
     },
     "execution_count": 764,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the vocabulary and document-term matrix together\n",
    "pd.DataFrame(simple_test_dtm.toarray(), columns=vect.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec45268a-bc57-4a34-b496-b30751119403",
   "metadata": {},
   "source": [
    "## Multinomial Naive Bayes Implementation\n",
    "\n",
    "- Your task is to implement Mutlinomial Naive Bayes from scratch, you can use numpy to vectorize your code and matplotlib  to show your analysis.\n",
    "- Below some information has given from the documentation about Multinomial Naive Bayes, this will give you some idea about using *Smoothing Priors*.\n",
    "- There is a sub-question for experimenting with $\\alpha > 0$, you don't have to implement it separetely, try to incomporate it in same Model Class / Function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c9e325-5e71-4810-83c9-a2e3b5a547b2",
   "metadata": {},
   "source": [
    "📌 From the scikit-learn documentation:\n",
    "\n",
    "- Multinomial Naive Bayes implements the naive Bayes algorithm for multinomially distributed data, and is one of the two classic naive Bayes variants used in text classification (where the data are typically represented as word vector counts, although tf-idf vectors are also known to work well in practice).\n",
    "\n",
    "- The distribution $\\theta_y = (\\theta_{y1}, \\theta_{y2}, \\dots, \\theta_{yn})$ is parametrized by vectors for each class $y$, where $n$ is the number of features (in text classification, the size of the vocabulary) and $\\theta_{yi}$ is the probability $P(x_i|y)$ of feature appearing in a sample belonging to class.\n",
    "\n",
    "- The parameters $\\theta_y$ is estimated by a smoothed version of maximum likelihood, i.e. relative frequency counting:\n",
    "\n",
    "$$\n",
    "\\hat{\\theta}_{yi} = \\frac{N_{yi} + \\alpha}{N_{y} + \\alpha n}\n",
    "$$\n",
    "\n",
    " where $N_{yi} = \\sum_{x \\in T}{x_i}$ is the number of times feature $i $ appears in a sample of class in the training set $T$, and $N_{y} = \\sum^{n}_{i=1}{N_{yi}}$ is the total count of all features for class $y$.\n",
    "\n",
    "- The smoothing priors $\\alpha \\gt 0$ accounts for features not present in the learning samples and **prevents zero probabilities** in further computations. Setting $\\alpha = 1$ is called Laplace smoothing, while $\\alpha \\lt 1$ is called Lidstone smoothing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14862c11-1b3c-4314-a64e-f98a185ce394",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your code here\n",
    "\"\"\"\n",
    "class NaiveBayes:\n",
    "    def __init__(self, alpha):\n",
    "        self.prior = None\n",
    "        self.likelihoods = None\n",
    "        self.classes = None\n",
    "        self.alpha = alpha\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        num_samples, num_features = X.shape[0], X.shape[1]\n",
    "        self.classes = np.unique(y)\n",
    "        total_classes = (self.classes.shape[0])\n",
    "        \n",
    "        # A prior for each class and a probability for each class\n",
    "        \n",
    "        self.priors = np.zeros(total_classes)\n",
    "        self.likelihoods = np.zeros((total_classes, num_features), dtype = float)\n",
    "        #print(self.priors)\n",
    "        for i in range(total_classes):\n",
    "            class_total = np.sum(y == self.classes[i])\n",
    "            word_count = np.sum(X[y == self.classes[i]], axis=0)\n",
    "            self.priors[i] = class_total / num_samples\n",
    "            self.likelihoods[i] = (word_count + self.alpha) / (class_total + num_features * self.alpha)\n",
    "        #print(self.priors)\n",
    "        #print(self.likelihoods)\n",
    "    \n",
    "    def predict(self, X):\n",
    "        y_est = [self.Predict_Single(i) for i in X]\n",
    "        return y_est\n",
    "    \n",
    "    def Predict_Single(self, x):\n",
    "        a = np.prod(self.likelihoods ** x, axis=1)\n",
    "        return np.argmax(self.priors * a)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dbcd1df-1c3d-477d-b0f6-c8c17868c797",
   "metadata": {},
   "source": [
    "## Vectorizing Training Sample\n",
    "\n",
    "- Use the Helper code above to vectorize for training samples\n",
    "- Don't overthink it, its very easy to do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e27f2f17",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['advance', 'again', 'and', 'can', 'for', 'have', 'help', 'here',\n",
       "       'if', 'in', 'it', 'lot', 'me', 'need', 'not', 'off', 'on',\n",
       "       'really', 'sure', 'thanks', 'this', 'tried', 'turning', 'why',\n",
       "       'with', 'you', 'your'], dtype=object)"
      ]
     },
     "execution_count": 766,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The next three cells, including this one are just for trying out the CountVectorizer. They do not mean anything for the assignment.\n",
    "\n",
    "train_phrases = [\"I'm not sure why I'm here\", \"I'm not sure if you can help me with this\", \"Have you tried turning it off and on again?\", \"I really need your help with this\", \"Thanks for your help in advance\", \"Thanks in advance\", \"Thanks\", \"Thanks a lot\", \"Thanks a lot in advance\", \"Thanks a lot in advance for your help\", \"Thanks a lot for your help\", \"Thanks\"]\n",
    "vec = CountVectorizer()\n",
    "train = vec.fit_transform(train_phrases)\n",
    "vec.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96c3c7f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>advance</th>\n",
       "      <th>again</th>\n",
       "      <th>and</th>\n",
       "      <th>can</th>\n",
       "      <th>for</th>\n",
       "      <th>have</th>\n",
       "      <th>help</th>\n",
       "      <th>here</th>\n",
       "      <th>if</th>\n",
       "      <th>in</th>\n",
       "      <th>...</th>\n",
       "      <th>really</th>\n",
       "      <th>sure</th>\n",
       "      <th>thanks</th>\n",
       "      <th>this</th>\n",
       "      <th>tried</th>\n",
       "      <th>turning</th>\n",
       "      <th>why</th>\n",
       "      <th>with</th>\n",
       "      <th>you</th>\n",
       "      <th>your</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>12 rows × 27 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    advance  again  and  can  for  have  help  here  if  in  ...  really  \\\n",
       "0         0      0    0    0    0     0     0     1   0   0  ...       0   \n",
       "1         0      0    0    1    0     0     1     0   1   0  ...       0   \n",
       "2         0      1    1    0    0     1     0     0   0   0  ...       0   \n",
       "3         0      0    0    0    0     0     1     0   0   0  ...       1   \n",
       "4         1      0    0    0    1     0     1     0   0   1  ...       0   \n",
       "5         1      0    0    0    0     0     0     0   0   1  ...       0   \n",
       "6         0      0    0    0    0     0     0     0   0   0  ...       0   \n",
       "7         0      0    0    0    0     0     0     0   0   0  ...       0   \n",
       "8         1      0    0    0    0     0     0     0   0   1  ...       0   \n",
       "9         1      0    0    0    1     0     1     0   0   1  ...       0   \n",
       "10        0      0    0    0    1     0     1     0   0   0  ...       0   \n",
       "11        0      0    0    0    0     0     0     0   0   0  ...       0   \n",
       "\n",
       "    sure  thanks  this  tried  turning  why  with  you  your  \n",
       "0      1       0     0      0        0    1     0    0     0  \n",
       "1      1       0     1      0        0    0     1    1     0  \n",
       "2      0       0     0      1        1    0     0    1     0  \n",
       "3      0       0     1      0        0    0     1    0     1  \n",
       "4      0       1     0      0        0    0     0    0     1  \n",
       "5      0       1     0      0        0    0     0    0     0  \n",
       "6      0       1     0      0        0    0     0    0     0  \n",
       "7      0       1     0      0        0    0     0    0     0  \n",
       "8      0       1     0      0        0    0     0    0     0  \n",
       "9      0       1     0      0        0    0     0    0     1  \n",
       "10     0       1     0      0        0    0     0    0     1  \n",
       "11     0       1     0      0        0    0     0    0     0  \n",
       "\n",
       "[12 rows x 27 columns]"
      ]
     },
     "execution_count": 767,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(train.toarray(), columns=vec.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2561b73-fc1b-47aa-8ae9-6d1ba8ad3015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4179, 7996)\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your code here\n",
    "\"\"\"\n",
    "# CountVectorizer for X_train\n",
    "X_trainlist = X_train.tolist()\n",
    "vec = CountVectorizer()\n",
    "X_train_dtm = vec.fit_transform(X_trainlist)\n",
    "X_train_dtm = X_train_dtm.toarray()\n",
    "\n",
    "# Vectorizing y_train\n",
    "\n",
    "y_trainlist = y_train.tolist()\n",
    "y_train_dtm = np.array(y_trainlist)\n",
    "print(X_train_dtm.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec22e087-4e03-4428-8f47-4cb4f2ec1645",
   "metadata": {},
   "source": [
    "## Calculate Priors and Estimate Model's performance on Training Sample\n",
    "\n",
    "- Calculate priors based on Training Sample using your NB implementation\n",
    "- Evaluate your model's performance on Training Data ($\\alpha = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1aa85aed-f38c-4acc-9c76-339d5914118d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Your code here\n",
    "\"\"\"\n",
    "Bayes = NaiveBayes(0)\n",
    "Bayes.fit(X_train_dtm, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8a6ec92-2ba4-40bf-9b5d-01825338c034",
   "metadata": {},
   "source": [
    "## Vectorizing Test Sample\n",
    "\n",
    "- Use the Training Sample vocabulary to create word count matrix for test samples\n",
    "- This is also shown in the Helper code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a16cc470-b033-444f-a366-5b3213eba877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1393, 7996)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your code here\n",
    "\"\"\"\n",
    "X_testlist = X_test.tolist()\n",
    "\n",
    "X_test_dtm = vec.transform(X_testlist)\n",
    "X_test_dtm = X_test_dtm.toarray()\n",
    "\n",
    "# Vectorizing y_test\n",
    "\n",
    "y_testlist = y_test.tolist()\n",
    "y_test_dtm = np.array(y_testlist)\n",
    "print(X_test_dtm.shape)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c02b84a-3116-4260-846d-50e6849efe48",
   "metadata": {},
   "source": [
    "## Estimate Model's performance on Test Sample\n",
    "\n",
    "- Evaluate your model's performance on Test Sample, using the Training Priors ($\\alpha = 0$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb763cb8-5a31-4d7d-b538-b741c8cfaae8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy is  0.9519023689877961\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your code here\n",
    "\"\"\"\n",
    "# Calculating the accuracy of the model\n",
    "y_est = Bayes.predict(X_test_dtm)\n",
    "print(\"Accuracy is \", np.mean(y_est == y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b415868e-4b61-4a13-abb5-22124e5312bc",
   "metadata": {},
   "source": [
    "## Select Smoothing Priors\n",
    "\n",
    "- Refactor your code to incorporate smoothing priors, select $\\alpha = 0$ for the previous estimates / sub-questions\n",
    "- Compare the performance with different values of $\\alpha \\gt 0$ as smoothing priors to take care of zero probabilities\n",
    "- You can display a Plot or Table to show the comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a707cc0d-1f16-40a5-93ec-0366165b077c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your code here\n",
    "\"\"\"\n",
    "alphas = np.linspace(1,50,100)\n",
    "accs = np.zeros(len(alphas))\n",
    "for i in range(len(alphas)):\n",
    "    nb2 = NaiveBayes(alphas[i])\n",
    "    nb2.fit(X_train_dtm, y_train)\n",
    "    y_est = nb2.predict(X_test_dtm)\n",
    "    accs[i] = np.mean(y_est == y_test)\n",
    "\n",
    "plt.plot(alphas, accs)\n",
    "plt.xlabel(\"Alpha value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d58051f8-67fb-4632-884b-23c719d4cae7",
   "metadata": {},
   "source": [
    "## Comparison with Sci-kit Learn Implementation\n",
    "\n",
    "- Use sci-kit learn's `sklearn.naive_bayes.MultinomialNB` model to compare your implementation's performance\n",
    "- (Optional) try other classifiers from `sklearn.naive_bayes` and see if you can make them work`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "192a26af-2b0a-4165-9d36-b5ed774c11fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(50,)\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_train_dtm' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\Chinmay\\Desktop\\3-2\\SMAI\\Assignments\\SMAI_Assignment_2\\2020102069_Assignment 2\\2020102069_Assignment 2\\Q3.ipynb Cell 39\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#X53sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(alpha)):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#X53sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     nb \u001b[39m=\u001b[39m MultinomialNB(fit_prior\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m, class_prior\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, alpha\u001b[39m=\u001b[39malpha[i])\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#X53sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     nb\u001b[39m.\u001b[39mfit(X_train_dtm, y_train)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#X53sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     y_est \u001b[39m=\u001b[39m nb\u001b[39m.\u001b[39mpredict(X_test_dtm)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/Chinmay/Desktop/3-2/SMAI/Assignments/SMAI_Assignment_2/2020102069_Assignment%202/2020102069_Assignment%202/Q3.ipynb#X53sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m     acc[i] \u001b[39m=\u001b[39m accuracy_score(y_test, y_est)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_dtm' is not defined"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Your code here\n",
    "\"\"\"\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "alpha = np.linspace(1,50,50)\n",
    "acc = np.zeros(len(alpha))\n",
    "print(alpha.shape)\n",
    "\n",
    "for i in range(len(alpha)):\n",
    "    nb = MultinomialNB(fit_prior=True, class_prior=None, alpha=alpha[i])\n",
    "    nb.fit(X_train_dtm, y_train)\n",
    "    y_est = nb.predict(X_test_dtm)\n",
    "    acc[i] = accuracy_score(y_test, y_est)\n",
    "\n",
    "plt.plot(alpha, acc)\n",
    "plt.xlabel(\"Alpha value\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"SKLearn Naive Bayes\")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "26de051ba29f2982a8de78e945f0abaf191376122a1563185a90213a26c5da77"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
